{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/cconsta1/dion-bone-classification/blob/main/5fold_strattified_dion.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6PiJOHXJulv"
      },
      "source": [
        "# Install Necessary Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6BxDHT3WJsz1"
      },
      "outputs": [],
      "source": [
        "# Install Necessary Libraries\n",
        "!pip install xgboost lightgbm openpyxl scikit-learn --quiet"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "O8TNHYKDJ1fQ"
      },
      "source": [
        "# Import Required Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Sd-crjyDfCzT"
      },
      "outputs": [],
      "source": [
        "# Import Libraries\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "from sklearn.model_selection import StratifiedKFold, LeaveOneOut\n",
        "from sklearn.metrics import classification_report, accuracy_score\n",
        "from sklearn.experimental import enable_iterative_imputer\n",
        "from sklearn.impute import SimpleImputer, KNNImputer, IterativeImputer\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "import openpyxl\n",
        "import xgboost as xgb\n",
        "import lightgbm as lgb\n",
        "from sklearn.ensemble import RandomForestClassifier\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "print(\"Libraries Imported!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "LI4QU0eoJ7FC"
      },
      "source": [
        "# Load and Prepare the Dataset"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "P1hHQjj3fCvs"
      },
      "outputs": [],
      "source": [
        "# Define File Paths (Modify as Needed)\n",
        "left_bones_url = \"https://raw.githubusercontent.com/cconsta1/dion-bone-classification/main/data/Cleaned_Left_Bones.csv\"\n",
        "right_bones_url = \"https://raw.githubusercontent.com/cconsta1/dion-bone-classification/main/data/Cleaned_Right_Bones.csv\"\n",
        "\n",
        "\n",
        "# Load Cleaned Datasets\n",
        "df_mf_left_bones = pd.read_csv(left_bones_url)\n",
        "df_mf_right_bones = pd.read_csv(right_bones_url)\n",
        "\n",
        "print(f\"Loaded Left Bones Dataset: {df_mf_left_bones.shape} | Right Bones Dataset: {df_mf_right_bones.shape}\")\n",
        "\n",
        "# Shuffle Datasets\n",
        "df_mf_left_bones = df_mf_left_bones.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_mf_right_bones = df_mf_right_bones.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "\n",
        "# Encode Target Variable (Pelvis Sex)\n",
        "df_mf_left_bones[\"Pelvis Sex\"] = LabelEncoder().fit_transform(df_mf_left_bones[\"Pelvis Sex\"])\n",
        "df_mf_right_bones[\"Pelvis Sex\"] = LabelEncoder().fit_transform(df_mf_right_bones[\"Pelvis Sex\"])\n",
        "\n",
        "print(\"Cleaned datasets loaded, shuffled, and encoded!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zySYMZ1dKAza"
      },
      "source": [
        "# Generate and Save Descriptive Statistics\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "vHyCp7trKzRD"
      },
      "outputs": [],
      "source": [
        "# Define file names and corresponding DataFrame selections\n",
        "tables_info = [\n",
        "    (\"Descriptive_Statistics_Left_Bones_Females.xlsx\", df_mf_left_bones[df_mf_left_bones[\"Pelvis Sex\"] == 0]),\n",
        "    (\"Descriptive_Statistics_Left_Bones_Males.xlsx\", df_mf_left_bones[df_mf_left_bones[\"Pelvis Sex\"] == 1]),\n",
        "    (\"Descriptive_Statistics_Right_Bones_Females.xlsx\", df_mf_right_bones[df_mf_right_bones[\"Pelvis Sex\"] == 0]),\n",
        "    (\"Descriptive_Statistics_Right_Bones_Males.xlsx\", df_mf_right_bones[df_mf_right_bones[\"Pelvis Sex\"] == 1])\n",
        "]\n",
        "\n",
        "# Loop through each case and save the descriptive statistics\n",
        "for file_name, df in tables_info:\n",
        "    stats = df.describe().transpose()\n",
        "    stats.to_excel(file_name)\n",
        "    print(f\"Saved: {file_name}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "3apucUh8fCkO"
      },
      "outputs": [],
      "source": [
        "print(\"Missing Values Per Column - Left Bones Dataset:\")\n",
        "print(df_mf_left_bones.isna().sum())\n",
        "\n",
        "print(\"\\n Missing Values Per Column - Right Bones Dataset:\")\n",
        "print(df_mf_right_bones.isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1dOAgHBCKIv8"
      },
      "source": [
        "# Imputation: Handling Missing Values"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "VbkOU84wCfs-"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.impute import KNNImputer, IterativeImputer\n",
        "\n",
        "# Define Imputation Methods\n",
        "knn_imputer = KNNImputer(n_neighbors=3)\n",
        "iter_imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
        "\n",
        "# Identify independent variables (exclude \"Pelvis Sex\")\n",
        "independent_vars = [col for col in df_mf_left_bones.columns if col != \"Pelvis Sex\"]\n",
        "\n",
        "# Create dictionary for datasets\n",
        "datasets = {\n",
        "    \"Original_L\": df_mf_left_bones.copy(),\n",
        "    \"Original_R\": df_mf_right_bones.copy(),\n",
        "    \"KNN_L\": df_mf_left_bones.copy(),\n",
        "    \"KNN_R\": df_mf_right_bones.copy(),\n",
        "    \"Iterative_L\": df_mf_left_bones.copy(),\n",
        "    \"Iterative_R\": df_mf_right_bones.copy(),\n",
        "}\n",
        "\n",
        "# Apply KNN & Iterative Imputation on the Original Datasets\n",
        "datasets[\"KNN_L\"][independent_vars] = knn_imputer.fit_transform(datasets[\"KNN_L\"][independent_vars])\n",
        "datasets[\"KNN_R\"][independent_vars] = knn_imputer.fit_transform(datasets[\"KNN_R\"][independent_vars])\n",
        "datasets[\"Iterative_L\"][independent_vars] = iter_imputer.fit_transform(datasets[\"Iterative_L\"][independent_vars])\n",
        "datasets[\"Iterative_R\"][independent_vars] = iter_imputer.fit_transform(datasets[\"Iterative_R\"][independent_vars])\n",
        "\n",
        "print(\"\\n Imputation Completed!\")\n",
        "print(f\" Available datasets: {list(datasets.keys())}\")\n",
        "print(\"\\n Missing Values After Imputation (Left - KNN):\")\n",
        "print(datasets[\"KNN_L\"].isna().sum())\n",
        "print(\"\\n Missing Values After Imputation (Right - Iterative):\")\n",
        "print(datasets[\"Iterative_R\"].isna().sum())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "K5EpDshvKSlV"
      },
      "source": [
        "# Wilcoxon Tests for Sex-Based Differences"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "wntGqF-RCfqB"
      },
      "outputs": [],
      "source": [
        "import scipy.stats as stats\n",
        "\n",
        "# Define output file paths\n",
        "wilcoxon_results_file = \"Wilcoxon_Test_Results.csv\"\n",
        "wilcoxon_results_by_bone_file = \"Wilcoxon_Test_Results_By_Bone.csv\"\n",
        "\n",
        "# Initialize results storage\n",
        "results = []\n",
        "results_by_bone = {}\n",
        "\n",
        "# Loop through all datasets and perform Wilcoxon test\n",
        "for dataset_name, df in datasets.items():\n",
        "    for var in independent_vars:\n",
        "        # Split data into Male (1) and Female (0) groups\n",
        "        males = df[df[\"Pelvis Sex\"] == 1][var].dropna()\n",
        "        females = df[df[\"Pelvis Sex\"] == 0][var].dropna()\n",
        "\n",
        "        num_males, num_females = len(males), len(females)\n",
        "\n",
        "        # Ensure both groups have enough samples for Wilcoxon test\n",
        "        if num_males > 1 and num_females > 1:\n",
        "            try:\n",
        "                stat, p_value = stats.mannwhitneyu(males, females, alternative=\"two-sided\")\n",
        "            except ValueError as e:\n",
        "                print(f\" Error for {var} in {dataset_name}: {e}\")\n",
        "                stat, p_value = None, None\n",
        "        else:\n",
        "            print(f\" Skipping {var} in {dataset_name} (Insufficient samples)\")\n",
        "            stat, p_value = None, None\n",
        "\n",
        "        # Store results in standard format (by dataset)\n",
        "        results.append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Variable\": var,\n",
        "            \"Wilcoxon Statistic\": stat,\n",
        "            \"p-value\": p_value,\n",
        "            \"Num Males\": num_males,\n",
        "            \"Num Females\": num_females\n",
        "        })\n",
        "\n",
        "        # Store results in grouped format (by bone)\n",
        "        if var not in results_by_bone:\n",
        "            results_by_bone[var] = []\n",
        "        results_by_bone[var].append({\n",
        "            \"Dataset\": dataset_name,\n",
        "            \"Wilcoxon Statistic\": stat,\n",
        "            \"p-value\": p_value,\n",
        "            \"Num Males\": num_males,\n",
        "            \"Num Females\": num_females\n",
        "        })\n",
        "\n",
        "# Convert standard results to DataFrame and save\n",
        "df_results = pd.DataFrame(results)\n",
        "df_results.to_csv(wilcoxon_results_file, index=False)\n",
        "\n",
        "print(\"\\n Wilcoxon test results saved!\")\n",
        "print(\"\\n Wilcoxon Test Results (Sample):\")\n",
        "print(df_results.head())\n",
        "\n",
        "# Convert grouped results (by bone) to DataFrame\n",
        "grouped_results = []\n",
        "for var, data in results_by_bone.items():\n",
        "    row = {\"Variable\": var}\n",
        "    for entry in data:\n",
        "        row[f\"{entry['Dataset']} Wilcoxon Statistic\"] = entry[\"Wilcoxon Statistic\"]\n",
        "        row[f\"{entry['Dataset']} p-value\"] = entry[\"p-value\"]\n",
        "        row[f\"{entry['Dataset']} Num Males\"] = entry[\"Num Males\"]\n",
        "        row[f\"{entry['Dataset']} Num Females\"] = entry[\"Num Females\"]\n",
        "    grouped_results.append(row)\n",
        "\n",
        "df_results_by_bone = pd.DataFrame(grouped_results)\n",
        "\n",
        "# Save the grouped results\n",
        "df_results_by_bone.to_csv(wilcoxon_results_by_bone_file, index=False)\n",
        "\n",
        "print(\"\\n Wilcoxon test results grouped by bone saved!\")\n",
        "print(\"\\n Sample of Wilcoxon results grouped by bone:\")\n",
        "print(df_results_by_bone.head())\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "8UpLwHmxKedB"
      },
      "source": [
        "# Filter Wilcoxon Test Results (p > 0.04)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ipI2p1wGCfmN"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define the file path (Adjust if necessary)\n",
        "file_path = \"/content/Wilcoxon_Test_Results_By_Bone.csv\"\n",
        "\n",
        "# Load the Wilcoxon test results file\n",
        "df = pd.read_csv(file_path)\n",
        "\n",
        "# Create an empty list to store filtered results\n",
        "filtered_results = []\n",
        "\n",
        "# Iterate through the DataFrame and filter for p-values > 0.04\n",
        "for index, row in df.iterrows():\n",
        "    for col in df.columns:\n",
        "        if \"p-value\" in col and row[col] > 0.04:\n",
        "            variable_name = row[\"Variable\"]\n",
        "            dataset_used = col.replace(\" p-value\", \"\")\n",
        "            filtered_results.append({\"Variable\": variable_name, \"Dataset\": dataset_used, \"p-value\": row[col]})\n",
        "\n",
        "# Convert the filtered results into a DataFrame and sort alphabetically\n",
        "df_filtered = pd.DataFrame(filtered_results).sort_values(by=[\"Variable\", \"Dataset\"])\n",
        "\n",
        "# Display the DataFrame\n",
        "print(df_filtered)\n",
        "\n",
        "# Optionally, save the filtered results to a new CSV file\n",
        "df_filtered.to_csv(\"/content/Filtered_Wilcoxon_Results.csv\", index=False)\n",
        "print(\"Filtered results saved to /content/Filtered_Wilcoxon_Results.csv\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UQS4Ryy6Kj09"
      },
      "source": [
        "# Define and Initialize Machine Learning Models"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "QPKo1NXJfKqw"
      },
      "outputs": [],
      "source": [
        "\n",
        "# Define Classifiers\n",
        "classifiers = {\n",
        "    \"XGBoost\": xgb.XGBClassifier(\n",
        "        eval_metric=\"logloss\",\n",
        "        random_state=42,\n",
        "        n_estimators=200,         # more trees (default is 100)\n",
        "        learning_rate=0.05,       # slower but more precise learning\n",
        "        max_depth=4,              # slightly deeper than default (3)\n",
        "        subsample=0.9,            # helps generalization\n",
        "        colsample_bytree=0.9,     # use 90% of features per tree\n",
        "        reg_alpha=0.1,            # L1 regularization\n",
        "        reg_lambda=1.0,           # L2 regularization\n",
        "        scale_pos_weight=1.66     # to balance 30 males vs. 18 females\n",
        "    ),\n",
        "\n",
        "    \"LightGBM\": lgb.LGBMClassifier(\n",
        "        n_estimators=100,\n",
        "        learning_rate=0.1,\n",
        "        max_depth=4,\n",
        "        min_child_samples=10,\n",
        "        reg_alpha=0.1,\n",
        "        reg_lambda=0.1,\n",
        "        class_weight='balanced',\n",
        "        random_state=42,\n",
        "        verbose=-1\n",
        "    ),\n",
        "\n",
        "    \"RandomForest\": RandomForestClassifier(\n",
        "        random_state=42\n",
        "    ),\n",
        "\n",
        "    \"LogReg\": LogisticRegression(\n",
        "        max_iter=1000,\n",
        "        random_state=42\n",
        "    )\n",
        "}\n",
        "\n",
        "print(\"Models Defined!\")"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uXbRFHufKm8A"
      },
      "source": [
        "# Set Up Cross-Validation Strategy"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "f-9LOye-fRlA"
      },
      "outputs": [],
      "source": [
        "def get_cv_strategy(y):\n",
        "    \"\"\"\n",
        "    Returns the appropriate cross-validation strategy:\n",
        "    - LOOCV if Male/Female count < 5\n",
        "    - 5-Fold Stratified CV otherwise\n",
        "    \"\"\"\n",
        "    min_class_size = y.value_counts().min()\n",
        "\n",
        "    if min_class_size < 5:\n",
        "        return LeaveOneOut(), \"Stratified LOOCV\"\n",
        "    else:\n",
        "        return StratifiedKFold(n_splits=min(5, min_class_size), shuffle=True, random_state=42), \"5-Fold Stratified CV\"\n",
        "\n",
        "print(\" Cross-Validation Strategies Defined!\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M3NKYR3gfW5r"
      },
      "outputs": [],
      "source": [
        "def run_classification(X, y, variable_name, model_name, dataset_name, cv_strategy, cv_type, results_df):\n",
        "    model = classifiers[model_name]\n",
        "    y_true, y_pred = [], []\n",
        "\n",
        "    for train_idx, test_idx in cv_strategy.split(X, y):\n",
        "        X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "        y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "        if len(np.unique(y_train)) < 2:\n",
        "            print(f\" Skipping {variable_name} ({model_name}) - Only one class present in training data!\")\n",
        "            return results_df\n",
        "\n",
        "        model.fit(X_train, y_train)\n",
        "        preds = model.predict(X_test)\n",
        "\n",
        "        y_true.extend(y_test)\n",
        "        y_pred.extend(preds)\n",
        "\n",
        "    report = classification_report(y_true, y_pred, output_dict=True, zero_division=0)\n",
        "\n",
        "    new_result = pd.DataFrame([{\n",
        "        \"Variable\": variable_name,\n",
        "        \"Model\": model_name,\n",
        "        \"Imputation\": dataset_name,\n",
        "        \"CV Strategy\": cv_type,\n",
        "        \"Accuracy\": accuracy_score(y_true, y_pred),\n",
        "        \"Precision M\": report[\"0\"][\"precision\"],\n",
        "        \"Precision F\": report[\"1\"][\"precision\"],\n",
        "        \"Recall M\": report[\"0\"][\"recall\"],\n",
        "        \"Recall F\": report[\"1\"][\"recall\"],\n",
        "        \"F1-score M\": report[\"0\"][\"f1-score\"],\n",
        "        \"F1-score F\": report[\"1\"][\"f1-score\"],\n",
        "        \"Sample size M\": report[\"0\"][\"support\"],\n",
        "        \"Sample size F\": report[\"1\"][\"support\"]\n",
        "    }])\n",
        "\n",
        "    results_df = pd.concat([results_df, new_result], ignore_index=True)\n",
        "\n",
        "    print(f\" Recorded results for {variable_name} using {model_name} ({cv_type})\")\n",
        "    return results_df\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rQr-49lnKvrB"
      },
      "source": [
        "# Train and Evaluate Models for Individual Bone Measurements\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lNWc1s2whwf5"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "\n",
        "# Define Folder for Plots\n",
        "plots_folder = \"/content/drive/MyDrive/Dion-results/Overfitting_Plots\"\n",
        "os.makedirs(plots_folder, exist_ok=True)\n",
        "\n",
        "# Generate Timestamp for File Naming\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "results_file = f\"/content/drive/MyDrive/Dion-results/Sex_Classification_{timestamp}.csv\"\n",
        "\n",
        "# Initialize Results DataFrame\n",
        "results_df = pd.DataFrame(columns=[\"Variable\", \"Model\", \"Imputation\", \"CV Strategy\", \"Accuracy\",\n",
        "                                   \"Precision M\", \"Precision F\", \"Recall M\", \"Recall F\",\n",
        "                                   \"F1-score M\", \"F1-score F\", \"Sample size M\", \"Sample size F\"])\n",
        "\n",
        "# Debug: Ensure datasets exist\n",
        "if not datasets:\n",
        "    print(\" ERROR: 'datasets' dictionary is empty! Check dataset loading.\")\n",
        "\n",
        "# Loop Over Each Dataset\n",
        "for dataset_name, df in datasets.items():\n",
        "    y = df[\"Pelvis Sex\"]\n",
        "\n",
        "    for variable in df.drop(columns=[\"Pelvis Sex\"]).columns:\n",
        "        X = df[[variable]]\n",
        "        cv_strategy, cv_type = get_cv_strategy(y)\n",
        "\n",
        "        for model_name in classifiers.keys():\n",
        "            # Boosting classifiers run on all datasets, RF & LogReg only on KNN & Iterative\n",
        "            if model_name in [\"XGBoost\", \"LightGBM\"] or dataset_name in [\"KNN_L\", \"KNN_R\", \"Iterative_L\", \"Iterative_R\"]:\n",
        "\n",
        "                # Debug: Ensure the function is called\n",
        "                print(f\" Running classification for: {variable} ({model_name}, {dataset_name})\")\n",
        "\n",
        "                # Run classification and collect results\n",
        "                results_df = run_classification(X, y, variable, model_name, dataset_name, cv_strategy, cv_type, results_df)\n",
        "\n",
        "                # Debug: Check if results_df is being updated\n",
        "                if results_df.empty:\n",
        "                    print(f\" ERROR: results_df is empty after processing {variable} ({model_name}, {dataset_name})\")\n",
        "\n",
        "                # Generate overfitting plots\n",
        "                train_scores, test_scores = [], []\n",
        "\n",
        "                for train_idx, test_idx in cv_strategy.split(X, y):\n",
        "                    X_train, X_test = X.iloc[train_idx], X.iloc[test_idx]\n",
        "                    y_train, y_test = y.iloc[train_idx], y.iloc[test_idx]\n",
        "\n",
        "                    if len(np.unique(y_train)) < 2:\n",
        "                        continue  # Skip iterations with only one class in training data\n",
        "\n",
        "                    model = classifiers[model_name]\n",
        "                    model.fit(X_train, y_train)\n",
        "\n",
        "                    train_scores.append(model.score(X_train, y_train))\n",
        "                    test_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "                # Plot Training vs. Test Scores\n",
        "                plt.figure(figsize=(8, 6))\n",
        "                plt.plot(range(len(train_scores)), train_scores, label=\"Train Score\", marker=\"o\", linestyle=\"-\")\n",
        "                plt.plot(range(len(test_scores)), test_scores, label=\"Test Score\", marker=\"o\", linestyle=\"--\")\n",
        "                plt.xlabel(\"Fold Number\")\n",
        "                plt.ylabel(\"Accuracy\")\n",
        "                plt.title(f\"Overfitting Analysis - {variable} ({model_name}, {dataset_name})\")\n",
        "                plt.legend()\n",
        "                plt.grid()\n",
        "\n",
        "                # Save Plot\n",
        "                plot_filename = f\"{plots_folder}/{variable}_{model_name}_{dataset_name}.png\"\n",
        "                plt.savefig(plot_filename)\n",
        "                plt.close()\n",
        "\n",
        "# Debug: Ensure DataFrame is not empty before saving\n",
        "if results_df.empty:\n",
        "    print(\" ERROR: results_df is empty! No results to save.\")\n",
        "else:\n",
        "    # Save Results\n",
        "    results_df.to_csv(results_file, index=False)\n",
        "    print(f\" Individual Variable Classification Results saved to {results_file}!\")\n",
        "\n",
        "# Debug: Verify File Exists\n",
        "if os.path.exists(results_file):\n",
        "    print(f\" CSV file successfully created at: {results_file}\")\n",
        "else:\n",
        "    print(\" ERROR: CSV file was not created!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "UstztkLFK3r7"
      },
      "source": [
        "# Train and Evaluate Models for Bone Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HCd1FpivIPC3"
      },
      "outputs": [],
      "source": [
        "import matplotlib.pyplot as plt\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from datetime import datetime\n",
        "\n",
        "# Define Bone Groups\n",
        "bone_groups = {\n",
        "    \"Clavicle\": [\"Clavicle maximum length\", \"Clavicle sagittal diameter at midshaft\", \"Clavicle vertical diameter at midshaft\"],\n",
        "    \"Scapula\": [\"Scapula height\", \"Scapula breadth\"],\n",
        "    \"Humerus\": [\"Humerus maximum length\", \"Humerus epicondylar breadth\", \"Humerus vertical diameter of head\",\n",
        "                \"Humerus maximum diameter at midshaft\", \"Humerus minimum diameter at midshaft\"],\n",
        "    \"Radius\": [\"Radius maximum length\", \"Radius sagittal diameter at midshaft\", \"Radius transverse diameter at midshaft\"],\n",
        "    \"Ulna\": [\"Ulna maximum length\", \"Ulna dorso-volar diameter\", \"Ulna transverse diameter\",\n",
        "             \"Ulna physiological length\", \"Ulna minimum circumference\"],\n",
        "    \"Femur\": [\"Femur maximum heigth\", \"Femur bicondylar length\", \"Femur epicondylar breadth\",\n",
        "              \"Femur maximum head diameter\", \"Femur sagittal subtrochanteric diameter\",\n",
        "              \"Femur transverse subtrochanteric diameter\", \"Femur sagittal midshaft diameter\",\n",
        "              \"Femur transverse midshaft diameter\", \"Femur midshaft circumference\"],\n",
        "    \"Tibia\": [\"Tibia length\", \"Tibia maximum proximal epiphyseal breadth\", \"Tibia maximum distal epiphyseal breadth\",\n",
        "              \"Tibia maximum diameter at the nutrient foramen\", \"Tibia transverse diameter at the nutrient foramen\",\n",
        "              \"Tibia circumference at the nutrient foramen\"],\n",
        "    \"Fibula\": [\"Fibula maximum length\", \"Fibula maximum diameter at midshaft\"],\n",
        "    \"Calcaneus\": [\"Calcaneus maximum length\", \"Calcaneus middle breadth\"]\n",
        "}\n",
        "\n",
        "# Define Folder for Bone Group Plots\n",
        "group_plots_folder = \"/content/drive/MyDrive/Dion-results/Overfitting_Plots_BoneGroups\"\n",
        "os.makedirs(group_plots_folder, exist_ok=True)\n",
        "\n",
        "# Generate Timestamp for File Naming\n",
        "timestamp = datetime.now().strftime(\"%Y%m%d_%H%M%S\")\n",
        "group_results_file = f\"/content/drive/MyDrive/Dion-results/Bone_Groups_{timestamp}.csv\"\n",
        "\n",
        "# Initialize Results DataFrame\n",
        "group_results_df = pd.DataFrame(columns=[\"Variable\", \"Model\", \"Imputation\", \"CV Strategy\", \"Accuracy\",\n",
        "                                         \"Precision M\", \"Precision F\", \"Recall M\", \"Recall F\",\n",
        "                                         \"F1-score M\", \"F1-score F\", \"Sample size M\", \"Sample size F\"])\n",
        "\n",
        "# Loop Over Each Dataset\n",
        "for dataset_name, df in datasets.items():\n",
        "    y = df[\"Pelvis Sex\"]\n",
        "\n",
        "    for group_name, variables in bone_groups.items():\n",
        "        # Ensure only available variables are used\n",
        "        valid_vars = [var for var in variables if var in df.columns]\n",
        "        if not valid_vars:\n",
        "            print(f\" Skipping {group_name} ({dataset_name}) - No valid data!\")\n",
        "            continue\n",
        "\n",
        "        X_group = df[valid_vars]\n",
        "        cv_strategy, cv_type = get_cv_strategy(y)\n",
        "\n",
        "        for model_name in classifiers.keys():\n",
        "            if model_name in [\"XGBoost\", \"LightGBM\"]:\n",
        "                print(f\" Running {model_name} on {group_name} ({dataset_name}) - Keeping NaNs\")\n",
        "                X_used = X_group\n",
        "                y_used = y\n",
        "            else:\n",
        "                if dataset_name not in [\"KNN_L\", \"KNN_R\", \"Iterative_L\", \"Iterative_R\"]:\n",
        "                    continue\n",
        "                X_used = X_group.dropna()\n",
        "                y_used = y.loc[X_used.index]\n",
        "\n",
        "                if len(X_used) == 0:\n",
        "                    print(f\" Skipping {group_name} ({dataset_name}) - No valid samples after dropping NaNs!\")\n",
        "                    continue\n",
        "\n",
        "                print(f\" Running {model_name} on {group_name} ({dataset_name}) - Dropping NaNs\")\n",
        "\n",
        "            # Run classification\n",
        "            group_results_df = run_classification(X_used, y_used, group_name, model_name, dataset_name,\n",
        "                                                  cv_strategy, cv_type, group_results_df)\n",
        "\n",
        "            # Overfitting Analysis\n",
        "            train_scores, test_scores = [], []\n",
        "\n",
        "            for train_idx, test_idx in cv_strategy.split(X_used, y_used):\n",
        "                X_train, X_test = X_used.iloc[train_idx], X_used.iloc[test_idx]\n",
        "                y_train, y_test = y_used.iloc[train_idx], y_used.iloc[test_idx]\n",
        "\n",
        "                if len(np.unique(y_train)) < 2:\n",
        "                    continue  # Skip if only one class present in fold\n",
        "\n",
        "                model = classifiers[model_name]\n",
        "                model.fit(X_train, y_train)\n",
        "                train_scores.append(model.score(X_train, y_train))\n",
        "                test_scores.append(model.score(X_test, y_test))\n",
        "\n",
        "            # Plot Train vs Test Accuracy\n",
        "            plt.figure(figsize=(8, 6))\n",
        "            plt.plot(range(len(train_scores)), train_scores, label=\"Train Score\", marker=\"o\", linestyle=\"-\")\n",
        "            plt.plot(range(len(test_scores)), test_scores, label=\"Test Score\", marker=\"o\", linestyle=\"--\")\n",
        "            plt.xlabel(\"Fold Number\")\n",
        "            plt.ylabel(\"Accuracy\")\n",
        "            plt.title(f\"Overfitting Analysis - {group_name} ({model_name}, {dataset_name})\")\n",
        "            plt.legend()\n",
        "            plt.grid()\n",
        "\n",
        "            plot_filename = f\"{group_plots_folder}/{group_name}_{model_name}_{dataset_name}.png\"\n",
        "            plt.savefig(plot_filename)\n",
        "            plt.close()\n",
        "\n",
        "# Save Results\n",
        "group_results_df.to_csv(group_results_file, index=False)\n",
        "print(f\" Bone Group Classification Results saved to {group_results_file}!\")\n",
        "print(f\" Overfitting Analysis Plots for Bone Groups saved in {group_plots_folder}!\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IFwOgd2eK98X"
      },
      "source": [
        "# Identify the Best Models for Individual Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "6siryIThOm9C"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file path for Sex Classification results\n",
        "\n",
        "# Copy the output file from above\n",
        "sex_classification_file = \"/content/drive/MyDrive/Dion-results/Sex_Classification_20250327_125312.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "df_sex = pd.read_csv(sex_classification_file)\n",
        "\n",
        "# Extract unique variable names (remove L/R labels)\n",
        "df_sex[\"Base Variable\"] = df_sex[\"Variable\"]\n",
        "\n",
        "# Identify the best classifier for each variable (Left and Right separately)\n",
        "top_models = []\n",
        "for var in df_sex[\"Base Variable\"].unique():\n",
        "    df_var = df_sex[df_sex[\"Base Variable\"] == var]\n",
        "\n",
        "    # Best model for Left (L)\n",
        "    df_L = df_var[df_var[\"Imputation\"].str.contains(\"_L\")]\n",
        "    if not df_L.empty:\n",
        "        top_L = df_L.loc[df_L[\"Accuracy\"].idxmax()]\n",
        "        top_models.append(top_L)\n",
        "\n",
        "    # Best model for Right (R)\n",
        "    df_R = df_var[df_var[\"Imputation\"].str.contains(\"_R\")]\n",
        "    if not df_R.empty:\n",
        "        top_R = df_R.loc[df_R[\"Accuracy\"].idxmax()]\n",
        "        top_models.append(top_R)\n",
        "\n",
        "# Create DataFrame with the top classifiers\n",
        "df_top_sex = pd.DataFrame(top_models)\n",
        "\n",
        "# Sort to keep Left & Right variables together\n",
        "df_top_sex = df_top_sex.sort_values(by=[\"Base Variable\", \"Imputation\"])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_top_sex = df_top_sex.drop(columns=[\"Base Variable\"])\n",
        "\n",
        "# Format floating-point numbers to two decimal places\n",
        "float_cols = [\"Accuracy\", \"Precision M\", \"Precision F\", \"Recall M\", \"Recall F\", \"F1-score M\", \"F1-score F\"]\n",
        "df_top_sex[float_cols] = df_top_sex[float_cols].applymap(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "# Display results\n",
        "print(\" Top classifiers for individual variables:\")\n",
        "print(df_top_sex)\n",
        "\n",
        "# Save results\n",
        "df_top_sex.to_csv(\"/content/Top_Individual_Variables.csv\", index=False)\n",
        "\n",
        "print(\" Processed and saved top classifiers for individual variables.\")\n",
        "\n",
        "# Generate LaTeX table\n",
        "latex_individual = df_top_sex.to_latex(index=False, column_format=\"lccccccccc\",\n",
        "                                       caption=\"Top Classifiers for Individual Variables (Left & Right)\",\n",
        "                                       label=\"tab:top_individual\")\n",
        "\n",
        "# Save LaTeX table\n",
        "with open(\"/content/Top_Individual_Variables.tex\", \"w\") as f:\n",
        "    f.write(latex_individual)\n",
        "\n",
        "print(\" LaTeX table generated! Download 'Top_Individual_Variables.tex' and insert into Overleaf.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N1RNhlfVLCPE"
      },
      "source": [
        "# Identify the Best Models for Bone Groups"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9R67uHxnjtTZ"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "\n",
        "# Define file path for Bone Group Classification results\n",
        "\n",
        "# Copy output file from above\n",
        "bone_groups_file = \"/content/drive/MyDrive/Dion-results/Bone_Groups_20250327_130942.csv\"\n",
        "\n",
        "# Load the CSV file\n",
        "df_bone = pd.read_csv(bone_groups_file)\n",
        "\n",
        "# Extract unique bone group names\n",
        "df_bone[\"Base Variable\"] = df_bone[\"Variable\"]\n",
        "\n",
        "# Identify the best classifier for each bone group (Left and Right separately)\n",
        "top_models = []\n",
        "for var in df_bone[\"Base Variable\"].unique():\n",
        "    df_var = df_bone[df_bone[\"Base Variable\"] == var]\n",
        "\n",
        "    # Best model for Left (L)\n",
        "    df_L = df_var[df_var[\"Imputation\"].str.contains(\"_L\")]\n",
        "    if not df_L.empty:\n",
        "        top_L = df_L.loc[df_L[\"Accuracy\"].idxmax()]\n",
        "        top_models.append(top_L)\n",
        "\n",
        "    # Best model for Right (R)\n",
        "    df_R = df_var[df_var[\"Imputation\"].str.contains(\"_R\")]\n",
        "    if not df_R.empty:\n",
        "        top_R = df_R.loc[df_R[\"Accuracy\"].idxmax()]\n",
        "        top_models.append(top_R)\n",
        "\n",
        "# Create DataFrame with the top classifiers\n",
        "df_top_bone = pd.DataFrame(top_models)\n",
        "\n",
        "# Sort to keep Left & Right bone groups together\n",
        "df_top_bone = df_top_bone.sort_values(by=[\"Base Variable\", \"Imputation\"])\n",
        "\n",
        "# Drop unnecessary columns\n",
        "df_top_bone = df_top_bone.drop(columns=[\"Base Variable\"])\n",
        "\n",
        "# Format floating-point numbers to two decimal places\n",
        "df_top_bone[float_cols] = df_top_bone[float_cols].applymap(lambda x: f\"{x:.2f}\")\n",
        "\n",
        "# Display results\n",
        "print(\" Top classifiers for bone groups:\")\n",
        "print(df_top_bone)\n",
        "\n",
        "# Save results\n",
        "df_top_bone.to_csv(\"/content/Top_Bone_Groups.csv\", index=False)\n",
        "\n",
        "print(\" Processed and saved top classifiers for bone groups.\")\n",
        "\n",
        "# Generate LaTeX table\n",
        "latex_bone = df_top_bone.to_latex(index=False, column_format=\"lccccccccc\",\n",
        "                                  caption=\"Top Classifiers for Bone Groups (Left & Right)\",\n",
        "                                  label=\"tab:top_bone_groups\")\n",
        "\n",
        "# Save LaTeX table\n",
        "with open(\"/content/Top_Bone_Groups.tex\", \"w\") as f:\n",
        "    f.write(latex_bone)\n",
        "\n",
        "print(\" LaTeX table generated! Download 'Top_Bone_Groups.tex' and insert into Overleaf.\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wbX8fvCP5v-a"
      },
      "source": [
        "# Train Logistic Regression Models on Bone Groups and Export to Text"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "keYoDHZrvTC1"
      },
      "outputs": [],
      "source": [
        "# Train and Export Logistic Regression Models for Bone Groups\n",
        "\n",
        "import os\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "from sklearn.linear_model import LogisticRegression\n",
        "\n",
        "# Output folder\n",
        "logreg_model_folder = \"/content/drive/MyDrive/Dion-results/Exported_LogReg_BoneGroups\"\n",
        "os.makedirs(logreg_model_folder, exist_ok=True)\n",
        "\n",
        "# Use only imputed datasets\n",
        "datasets_to_use = [\"KNN_L\", \"KNN_R\", \"Iterative_L\", \"Iterative_R\"]\n",
        "\n",
        "# Bone group definitions\n",
        "bone_groups = {\n",
        "    \"Clavicle\": [\"Clavicle maximum length\", \"Clavicle sagittal diameter at midshaft\", \"Clavicle vertical diameter at midshaft\"],\n",
        "    \"Scapula\": [\"Scapula height\", \"Scapula breadth\"],\n",
        "    \"Humerus\": [\"Humerus maximum length\", \"Humerus epicondylar breadth\", \"Humerus vertical diameter of head\",\n",
        "                \"Humerus maximum diameter at midshaft\", \"Humerus minimum diameter at midshaft\"],\n",
        "    \"Radius\": [\"Radius maximum length\", \"Radius sagittal diameter at midshaft\", \"Radius transverse diameter at midshaft\"],\n",
        "    \"Ulna\": [\"Ulna maximum length\", \"Ulna dorso-volar diameter\", \"Ulna transverse diameter\",\n",
        "             \"Ulna physiological length\", \"Ulna minimum circumference\"],\n",
        "    \"Femur\": [\"Femur maximum heigth\", \"Femur bicondylar length\", \"Femur epicondylar breadth\",\n",
        "              \"Femur maximum head diameter\", \"Femur sagittal subtrochanteric diameter\",\n",
        "              \"Femur transverse subtrochanteric diameter\", \"Femur sagittal midshaft diameter\",\n",
        "              \"Femur transverse midshaft diameter\", \"Femur midshaft circumference\"],\n",
        "    \"Tibia\": [\"Tibia length\", \"Tibia maximum proximal epiphyseal breadth\", \"Tibia maximum distal epiphyseal breadth\",\n",
        "              \"Tibia maximum diameter at the nutrient foramen\", \"Tibia transverse diameter at the nutrient foramen\",\n",
        "              \"Tibia circumference at the nutrient foramen\"],\n",
        "    \"Fibula\": [\"Fibula maximum length\", \"Fibula maximum diameter at midshaft\"],\n",
        "    \"Calcaneus\": [\"Calcaneus maximum length\", \"Calcaneus middle breadth\"]\n",
        "}\n",
        "\n",
        "# Loop through datasets and bone groups\n",
        "for dataset_name, df in datasets.items():\n",
        "    if dataset_name not in datasets_to_use:\n",
        "        continue\n",
        "\n",
        "    y = df[\"Pelvis Sex\"]\n",
        "\n",
        "    for group_name, features in bone_groups.items():\n",
        "        valid_features = [f for f in features if f in df.columns]\n",
        "        if not valid_features:\n",
        "            print(f\"Skipping {group_name} ({dataset_name}) - No valid features\")\n",
        "            continue\n",
        "\n",
        "        X = df[valid_features].dropna()\n",
        "        y_subset = y.loc[X.index]\n",
        "\n",
        "        if len(X) < 2 or len(np.unique(y_subset)) < 2:\n",
        "            print(f\"Skipping {group_name} ({dataset_name}) - Not enough data or only one class\")\n",
        "            continue\n",
        "\n",
        "        # Train on all available data\n",
        "        model = LogisticRegression(max_iter=1000, random_state=42)\n",
        "        model.fit(X, y_subset)\n",
        "\n",
        "        intercept = model.intercept_[0]\n",
        "        coefs = model.coef_[0]\n",
        "\n",
        "        file_path = os.path.join(logreg_model_folder, f\"LogisticRegression_{group_name}_{dataset_name}.txt\")\n",
        "        with open(file_path, \"w\") as f:\n",
        "            f.write(f\"Logistic Regression Model for {group_name} ({dataset_name})\\n\\n\")\n",
        "            f.write(\"Binary classification: 0 = Female, 1 = Male\\n\")\n",
        "            f.write(f\"Samples used: {len(X)}\\n\\n\")\n",
        "\n",
        "            f.write(f\"Intercept: {intercept:.6f}\\n\\n\")\n",
        "            f.write(\"Coefficients:\\n\")\n",
        "            for feature, coef in zip(valid_features, coefs):\n",
        "                f.write(f\"  {feature}: {coef:.6f}\\n\")\n",
        "\n",
        "            f.write(\"\\nFeature Order:\\n\")\n",
        "            f.write(\", \".join(valid_features) + \"\\n\\n\")\n",
        "\n",
        "            f.write(\"Classification Rule:\\n\")\n",
        "            f.write(\"Compute the score using your measurements:\\n\\n\")\n",
        "            formula = f\"{intercept:.3f} + \" + \" + \".join([f\"{coef:.3f} × {feat}\" for coef, feat in zip(coefs, valid_features)])\n",
        "            f.write(f\"  Score = {formula}\\n\\n\")\n",
        "            f.write(\"Then apply the rule:\\n\")\n",
        "            f.write(\"  If Score > 0 → Classify as Male\\n\")\n",
        "            f.write(\"  If Score ≤ 0 → Classify as Female\\n\\n\")\n",
        "\n",
        "            f.write(\"Example (replace values with your measurements):\\n\")\n",
        "            example = f\"{intercept:.3f}\"\n",
        "            for coef, feat in zip(coefs, valid_features):\n",
        "                example += f\" + ({coef:.3f} × [your {feat}])\"\n",
        "            f.write(f\"  Score = {example}\\n\")\n",
        "\n",
        "        print(f\"Exported: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_0LnSYScEoAR"
      },
      "source": [
        "# Train and Export Boosted Models for Bone Groups (as Pickle Files)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "M9AjUnmAvSrS"
      },
      "outputs": [],
      "source": [
        "# Train and Export XGBoost, LightGBM, and RandomForest Models for Bone Groups\n",
        "\n",
        "import os\n",
        "import joblib\n",
        "import numpy as np\n",
        "import pandas as pd\n",
        "\n",
        "# Output folder\n",
        "boosted_model_folder = \"/content/drive/MyDrive/Dion-results/Exported_Pickled_BoostedModels\"\n",
        "os.makedirs(boosted_model_folder, exist_ok=True)\n",
        "\n",
        "# Datasets\n",
        "datasets_rf = [\"KNN_L\", \"KNN_R\", \"Iterative_L\", \"Iterative_R\"]\n",
        "datasets_boosting = [\"Original_L\", \"Original_R\", \"KNN_L\", \"KNN_R\", \"Iterative_L\", \"Iterative_R\"]\n",
        "\n",
        "# Bone group definitions\n",
        "bone_groups = {\n",
        "    \"Clavicle\": [\"Clavicle maximum length\", \"Clavicle sagittal diameter at midshaft\", \"Clavicle vertical diameter at midshaft\"],\n",
        "    \"Scapula\": [\"Scapula height\", \"Scapula breadth\"],\n",
        "    \"Humerus\": [\"Humerus maximum length\", \"Humerus epicondylar breadth\", \"Humerus vertical diameter of head\",\n",
        "                \"Humerus maximum diameter at midshaft\", \"Humerus minimum diameter at midshaft\"],\n",
        "    \"Radius\": [\"Radius maximum length\", \"Radius sagittal diameter at midshaft\", \"Radius transverse diameter at midshaft\"],\n",
        "    \"Ulna\": [\"Ulna maximum length\", \"Ulna dorso-volar diameter\", \"Ulna transverse diameter\",\n",
        "             \"Ulna physiological length\", \"Ulna minimum circumference\"],\n",
        "    \"Femur\": [\"Femur maximum heigth\", \"Femur bicondylar length\", \"Femur epicondylar breadth\",\n",
        "              \"Femur maximum head diameter\", \"Femur sagittal subtrochanteric diameter\",\n",
        "              \"Femur transverse subtrochanteric diameter\", \"Femur sagittal midshaft diameter\",\n",
        "              \"Femur transverse midshaft diameter\", \"Femur midshaft circumference\"],\n",
        "    \"Tibia\": [\"Tibia length\", \"Tibia maximum proximal epiphyseal breadth\", \"Tibia maximum distal epiphyseal breadth\",\n",
        "              \"Tibia maximum diameter at the nutrient foramen\", \"Tibia transverse diameter at the nutrient foramen\",\n",
        "              \"Tibia circumference at the nutrient foramen\"],\n",
        "    \"Fibula\": [\"Fibula maximum length\", \"Fibula maximum diameter at midshaft\"],\n",
        "    \"Calcaneus\": [\"Calcaneus maximum length\", \"Calcaneus middle breadth\"]\n",
        "}\n",
        "\n",
        "# Loop through datasets and train/export models\n",
        "for dataset_name, df in datasets.items():\n",
        "    y = df[\"Pelvis Sex\"]\n",
        "\n",
        "    for group_name, features in bone_groups.items():\n",
        "        valid_features = [f for f in features if f in df.columns]\n",
        "        if not valid_features:\n",
        "            continue\n",
        "\n",
        "        X = df[valid_features].dropna()\n",
        "        y_subset = y.loc[X.index]\n",
        "\n",
        "        if len(X) < 2 or len(np.unique(y_subset)) < 2:\n",
        "            continue\n",
        "\n",
        "        for model_name in [\"XGBoost\", \"LightGBM\", \"RandomForest\"]:\n",
        "            if model_name == \"RandomForest\" and dataset_name not in datasets_rf:\n",
        "                continue\n",
        "            if model_name in [\"XGBoost\", \"LightGBM\"] and dataset_name not in datasets_boosting:\n",
        "                continue\n",
        "\n",
        "            model = classifiers[model_name]\n",
        "            model.fit(X, y_subset)\n",
        "\n",
        "            filename = f\"{model_name}_{group_name}_{dataset_name}.pkl\"\n",
        "            file_path = os.path.join(boosted_model_folder, filename)\n",
        "            joblib.dump(model, file_path)\n",
        "            print(f\"Exported: {file_path}\")\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ATqLDF-iICrx"
      },
      "source": [
        "# Load and Test Pickled XGBoost, LightGBM, and RandomForest Models\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "sEFB-7IqH6Hj"
      },
      "outputs": [],
      "source": [
        "import os\n",
        "import joblib\n",
        "import pandas as pd\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "# Path to the pickled models\n",
        "model_dir = \"/content/drive/MyDrive/Dion-results/Exported_Pickled_BoostedModels\"\n",
        "\n",
        "# Load datasets again (already preprocessed in earlier cells)\n",
        "left_bones_path = \"/content/drive/MyDrive/Dion-data/Cleaned_Left_Bones.csv\"\n",
        "right_bones_path = \"/content/drive/MyDrive/Dion-data/Cleaned_Right_Bones.csv\"\n",
        "df_mf_left_bones = pd.read_csv(left_bones_path)\n",
        "df_mf_right_bones = pd.read_csv(right_bones_path)\n",
        "\n",
        "df_mf_left_bones = df_mf_left_bones.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_mf_right_bones = df_mf_right_bones.sample(frac=1, random_state=42).reset_index(drop=True)\n",
        "df_mf_left_bones[\"Pelvis Sex\"] = LabelEncoder().fit_transform(df_mf_left_bones[\"Pelvis Sex\"])\n",
        "df_mf_right_bones[\"Pelvis Sex\"] = LabelEncoder().fit_transform(df_mf_right_bones[\"Pelvis Sex\"])\n",
        "\n",
        "# Load imputed datasets (KNN & Iterative)\n",
        "from sklearn.impute import KNNImputer, IterativeImputer\n",
        "independent_vars = [col for col in df_mf_left_bones.columns if col != \"Pelvis Sex\"]\n",
        "knn_imputer = KNNImputer(n_neighbors=3)\n",
        "iter_imputer = IterativeImputer(max_iter=1000, random_state=42)\n",
        "\n",
        "datasets = {\n",
        "    \"Original_L\": df_mf_left_bones.copy(),\n",
        "    \"Original_R\": df_mf_right_bones.copy(),\n",
        "    \"KNN_L\": df_mf_left_bones.copy(),\n",
        "    \"KNN_R\": df_mf_right_bones.copy(),\n",
        "    \"Iterative_L\": df_mf_left_bones.copy(),\n",
        "    \"Iterative_R\": df_mf_right_bones.copy(),\n",
        "}\n",
        "datasets[\"KNN_L\"][independent_vars] = knn_imputer.fit_transform(datasets[\"KNN_L\"][independent_vars])\n",
        "datasets[\"KNN_R\"][independent_vars] = knn_imputer.fit_transform(datasets[\"KNN_R\"][independent_vars])\n",
        "datasets[\"Iterative_L\"][independent_vars] = iter_imputer.fit_transform(datasets[\"Iterative_L\"][independent_vars])\n",
        "datasets[\"Iterative_R\"][independent_vars] = iter_imputer.fit_transform(datasets[\"Iterative_R\"][independent_vars])\n",
        "\n",
        "# Bone group definitions\n",
        "bone_groups = {\n",
        "    \"Clavicle\": [\"Clavicle maximum length\", \"Clavicle sagittal diameter at midshaft\", \"Clavicle vertical diameter at midshaft\"],\n",
        "    \"Scapula\": [\"Scapula height\", \"Scapula breadth\"],\n",
        "    \"Humerus\": [\"Humerus maximum length\", \"Humerus epicondylar breadth\", \"Humerus vertical diameter of head\",\n",
        "                \"Humerus maximum diameter at midshaft\", \"Humerus minimum diameter at midshaft\"],\n",
        "    \"Radius\": [\"Radius maximum length\", \"Radius sagittal diameter at midshaft\", \"Radius transverse diameter at midshaft\"],\n",
        "    \"Ulna\": [\"Ulna maximum length\", \"Ulna dorso-volar diameter\", \"Ulna transverse diameter\",\n",
        "             \"Ulna physiological length\", \"Ulna minimum circumference\"],\n",
        "    \"Femur\": [\"Femur maximum heigth\", \"Femur bicondylar length\", \"Femur epicondylar breadth\",\n",
        "              \"Femur maximum head diameter\", \"Femur sagittal subtrochanteric diameter\",\n",
        "              \"Femur transverse subtrochanteric diameter\", \"Femur sagittal midshaft diameter\",\n",
        "              \"Femur transverse midshaft diameter\", \"Femur midshaft circumference\"],\n",
        "    \"Tibia\": [\"Tibia length\", \"Tibia maximum proximal epiphyseal breadth\", \"Tibia maximum distal epiphyseal breadth\",\n",
        "              \"Tibia maximum diameter at the nutrient foramen\", \"Tibia transverse diameter at the nutrient foramen\",\n",
        "              \"Tibia circumference at the nutrient foramen\"],\n",
        "    \"Fibula\": [\"Fibula maximum length\", \"Fibula maximum diameter at midshaft\"],\n",
        "    \"Calcaneus\": [\"Calcaneus maximum length\", \"Calcaneus middle breadth\"]\n",
        "}\n",
        "\n",
        "# Evaluate all models\n",
        "results = []\n",
        "\n",
        "for filename in os.listdir(model_dir):\n",
        "    if not filename.endswith(\".pkl\"):\n",
        "        continue\n",
        "\n",
        "    model_path = os.path.join(model_dir, filename)\n",
        "    model = joblib.load(model_path)\n",
        "\n",
        "    try:\n",
        "        model_name, bone_group, dataset_name = filename.replace(\".pkl\", \"\").split(\"_\", 2)\n",
        "    except ValueError:\n",
        "        print(f\"Skipping invalid filename: {filename}\")\n",
        "        continue\n",
        "\n",
        "    if dataset_name not in datasets:\n",
        "        print(f\"Dataset {dataset_name} not found for model {filename}\")\n",
        "        continue\n",
        "\n",
        "    df = datasets[dataset_name]\n",
        "    if bone_group not in bone_groups:\n",
        "        print(f\"Bone group {bone_group} not in defined groups for {filename}\")\n",
        "        continue\n",
        "\n",
        "    features = [f for f in bone_groups[bone_group] if f in df.columns]\n",
        "    if not features:\n",
        "        print(f\"No valid features in {filename}\")\n",
        "        continue\n",
        "\n",
        "    X = df[features].dropna()\n",
        "    y = df.loc[X.index, \"Pelvis Sex\"]\n",
        "\n",
        "    if len(X) < 2 or len(np.unique(y)) < 2:\n",
        "        continue\n",
        "\n",
        "    y_pred = model.predict(X)\n",
        "    acc = accuracy_score(y, y_pred)\n",
        "    results.append({\n",
        "        \"Model\": model_name,\n",
        "        \"Bone Group\": bone_group,\n",
        "        \"Dataset\": dataset_name,\n",
        "        \"Accuracy\": acc\n",
        "    })\n",
        "\n",
        "# Display summary\n",
        "df_results = pd.DataFrame(results).sort_values(by=\"Accuracy\", ascending=False)\n",
        "print(df_results.sample(50))\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "kr3-YCIZMDvB"
      },
      "source": [
        "# Sample CSV file"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "eIGenAzLIZBl"
      },
      "outputs": [],
      "source": [
        "import pandas as pd\n",
        "import os\n",
        "\n",
        "# Example: Humerus bone group (used by many models)\n",
        "sample_data = {\n",
        "    \"Humerus maximum length\": [310.0, 280.5],\n",
        "    \"Humerus epicondylar breadth\": [66.2, 58.3],\n",
        "    \"Humerus vertical diameter of head\": [43.1, 39.8],\n",
        "    \"Humerus maximum diameter at midshaft\": [23.0, 20.7],\n",
        "    \"Humerus minimum diameter at midshaft\": [20.1, 18.9]\n",
        "}\n",
        "\n",
        "# Create DataFrame\n",
        "df_sample = pd.DataFrame(sample_data)\n",
        "\n",
        "# Define file path\n",
        "output_path = \"/content/sample_data_for_prediction.csv\"\n",
        "\n",
        "# Save CSV\n",
        "df_sample.to_csv(output_path, index=False)\n",
        "print(f\"Sample CSV saved to: {output_path}\")\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "tcfbNB-RMG05"
      },
      "outputs": [],
      "source": []
    }
  ],
  "metadata": {
    "colab": {
      "authorship_tag": "ABX9TyPOgI2nXQb8ZycFB3QkfySa",
      "include_colab_link": true,
      "machine_shape": "hm",
      "mount_file_id": "https://github.com/cconsta1/dion-bone-classification/blob/main/5fold_strattified_dion.ipynb",
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
